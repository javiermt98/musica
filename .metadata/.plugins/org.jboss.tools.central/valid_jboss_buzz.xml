<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">RESTEasy 3.15.0.Final is now available</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/hjNEeeR06DE/" /><author><name /></author><id>https://resteasy.github.io/2021/02/18/resteasy-3.15.0.Final/</id><updated>2021-02-18T19:52:00Z</updated><dc:creator /><summary type="html">&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/hjNEeeR06DE" height="1" width="1" alt=""/&gt;</summary><feedburner:origLink>https://resteasy.github.io/2021/02/18/resteasy-3.15.0.Final/</feedburner:origLink></entry><entry><title type="html">Byteman 4.0.14 has been released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ImgC5mf259o/byteman-4014-has-been-released.html" /><author><name>Andrew Dinn</name></author><id>http://bytemanblog.blogspot.com/2021/02/byteman-4014-has-been-released.html</id><updated>2021-02-18T14:37:00Z</updated><content type="html">Byteman 4.0.14 is now available from the and from the . It is the latest update release for use on all JDK9+ runtimes. It is also recommended as the preferred release for use on JDK8- runtimes. Byteman 4.0.14 is a maintenance release which fixes a few minor issues. More details are provided in the .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ImgC5mf259o" height="1" width="1" alt=""/&gt;</content><dc:creator>Andrew Dinn</dc:creator><feedburner:origLink>http://bytemanblog.blogspot.com/2021/02/byteman-4014-has-been-released.html</feedburner:origLink></entry><entry><title>How to work around Docker’s new download rate limit on Red Hat OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/EKHJid8FAQ8/" /><category term="Containers" /><category term="Kubernetes" /><category term="Open source" /><author><name>Joel Lord</name></author><id>https://developers.redhat.com/blog/?p=870297</id><updated>2021-02-18T08:00:17Z</updated><published>2021-02-18T08:00:17Z</published><content type="html">&lt;p&gt;Have you recently tried running &lt;code&gt;oc new-app &amp;#60;docker-image&amp;#62;&lt;/code&gt;on &lt;a href="https://developers.redhat.com/products/openshift/getting-started"&gt;Red Hat OpenShift&lt;/a&gt; and received a similar error message to the one below?&lt;/p&gt; &lt;pre&gt;W0216 12:21:52.014221 671649 dockerimagelookup.go:237] container image registry lookup failed: docker.io/username/image:latest: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit &lt;/pre&gt; &lt;p&gt;If so, you &lt;strong&gt;do not need to upgrade your Docker account to a paid one&lt;/strong&gt;. Instead, you can use a secret to pull your images as an authenticated Docker Hub user.&lt;/p&gt; &lt;h2&gt;Docker&amp;#8217;s new rate limit&lt;/h2&gt; &lt;p&gt;Docker recently &lt;a target="_blank" rel="nofollow" href="https://www.docker.com/increase-rate-limits"&gt;changed its policy&lt;/a&gt; for downloading images as an anonymous user. The company now has a limit of 100 downloads every six hours from a single IP address.&lt;/p&gt; &lt;p&gt;If you are using the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;OpenShift Developer Sandbox&lt;/a&gt; to experiment with a free OpenShift cluster, like I was recently, then you might encounter the error message shown in Figure 1.&lt;/p&gt; &lt;div id="attachment_870327" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/2021-02-16_rate_limit.png"&gt;&lt;img aria-describedby="caption-attachment-870327" class="wp-image-870327 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/2021-02-16_rate_limit-1024x278.png" alt="" width="640" height="174" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/02/2021-02-16_rate_limit-1024x278.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/2021-02-16_rate_limit-300x81.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/2021-02-16_rate_limit-768x208.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/2021-02-16_rate_limit.png 1132w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-870327" class="wp-caption-text"&gt;Figure 1: The new rate limit error message from Docker.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;You might receive this error message after trying to create a new application with the &lt;code&gt;$ oc new-app&lt;/code&gt; command or from the user interface (UI). The issue is that many users are using the same cluster at the same time. Whenever someone tries to create a new application from a Docker image, the cluster downloads the image as an anonymous user, which counts toward the new rate limit. Eventually, the limit is reached, and the error message pops up.&lt;/p&gt; &lt;p&gt;Fortunately, the workaround is easy.&lt;/p&gt; &lt;h2&gt;Authenticate to your Docker Hub account&lt;/h2&gt; &lt;p&gt;All you have to do to avoid Docker&amp;#8217;s new rate-limit error is authenticate to your Docker Hub account. After you&amp;#8217;ve authenticated to the account, you won&amp;#8217;t be pulling the image as an anonymous user but as an authenticated user. The image download will count against your personal limit of 200 downloads per six hours instead of the 100 downloads shared across all anonymous cluster users.&lt;/p&gt; &lt;p&gt;You can use the following command to authenticate:&lt;/p&gt; &lt;pre&gt;$ oc create secret docker-registry docker --docker-server=docker.io --docker-username=&amp;#60;username&amp;#62; --docker-password=&amp;#60;password&amp;#62; --docker-email=&amp;#60;email&amp;#62; $ oc secrets link default docker --for=pull $ oc new-app &amp;#60;username&amp;#62;/&amp;#60;image&amp;#62; --source-secret=docker &lt;/pre&gt; &lt;p&gt;Note that it is recommended that you &lt;a target="_blank" rel="nofollow" href="https://docs.docker.com/docker-hub/access-tokens/"&gt;use an access token&lt;/a&gt; here instead of your actual password. Using an access token is also the only way to authenticate if you have two-factor authentication set up on your account.&lt;/p&gt; &lt;p&gt;If you prefer to use the UI, as I do, click &lt;b&gt;Create an image pull secret&lt;/b&gt;, as shown in Figure 2.&lt;/p&gt; &lt;div id="attachment_870337" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2021/02/2021-02-16_pull_secret.png"&gt;&lt;img aria-describedby="caption-attachment-870337" class="wp-image-870337 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2021/02/2021-02-16_pull_secret-1024x480.png" alt="" width="640" height="300" srcset="https://developers.redhat.com/blog/wp-content/uploads/2021/02/2021-02-16_pull_secret-1024x480.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/2021-02-16_pull_secret-300x141.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/2021-02-16_pull_secret-768x360.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2021/02/2021-02-16_pull_secret.png 1238w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-870337" class="wp-caption-text"&gt;Figure 2: Adding a pull secret from the Docker UI.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Either way, you can quickly create an image pull secret, authenticate to your Docker Hub account, and get back to experimenting in the OpenShift Developer Sandbox.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Docker&amp;#8217;s new download rate limit has caught a few of us by surprise, but the workaround is easy. This article showed you how to use a secret to pull your images as an authenticated Docker Hub user. Once you&amp;#8217;ve done that, you will be able to download images without hitting the rate limit error.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F18%2Fhow-to-work-around-dockers-new-download-rate-limit-on-red-hat-openshift%2F&amp;#38;linkname=How%20to%20work%20around%20Docker%E2%80%99s%20new%20download%20rate%20limit%20on%20Red%20Hat%20OpenShift" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F18%2Fhow-to-work-around-dockers-new-download-rate-limit-on-red-hat-openshift%2F&amp;#38;linkname=How%20to%20work%20around%20Docker%E2%80%99s%20new%20download%20rate%20limit%20on%20Red%20Hat%20OpenShift" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F18%2Fhow-to-work-around-dockers-new-download-rate-limit-on-red-hat-openshift%2F&amp;#38;linkname=How%20to%20work%20around%20Docker%E2%80%99s%20new%20download%20rate%20limit%20on%20Red%20Hat%20OpenShift" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F18%2Fhow-to-work-around-dockers-new-download-rate-limit-on-red-hat-openshift%2F&amp;#38;linkname=How%20to%20work%20around%20Docker%E2%80%99s%20new%20download%20rate%20limit%20on%20Red%20Hat%20OpenShift" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F18%2Fhow-to-work-around-dockers-new-download-rate-limit-on-red-hat-openshift%2F&amp;#38;linkname=How%20to%20work%20around%20Docker%E2%80%99s%20new%20download%20rate%20limit%20on%20Red%20Hat%20OpenShift" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F18%2Fhow-to-work-around-dockers-new-download-rate-limit-on-red-hat-openshift%2F&amp;#38;linkname=How%20to%20work%20around%20Docker%E2%80%99s%20new%20download%20rate%20limit%20on%20Red%20Hat%20OpenShift" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F18%2Fhow-to-work-around-dockers-new-download-rate-limit-on-red-hat-openshift%2F&amp;#38;linkname=How%20to%20work%20around%20Docker%E2%80%99s%20new%20download%20rate%20limit%20on%20Red%20Hat%20OpenShift" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F18%2Fhow-to-work-around-dockers-new-download-rate-limit-on-red-hat-openshift%2F&amp;#038;title=How%20to%20work%20around%20Docker%E2%80%99s%20new%20download%20rate%20limit%20on%20Red%20Hat%20OpenShift" data-a2a-url="https://developers.redhat.com/blog/2021/02/18/how-to-work-around-dockers-new-download-rate-limit-on-red-hat-openshift/" data-a2a-title="How to work around Docker’s new download rate limit on Red Hat OpenShift"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/02/18/how-to-work-around-dockers-new-download-rate-limit-on-red-hat-openshift/"&gt;How to work around Docker&amp;#8217;s new download rate limit on Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/EKHJid8FAQ8" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Have you recently tried running oc new-app &amp;#60;docker-image&amp;#62;on Red Hat OpenShift and received a similar error message to the one below? W0216 12:21:52.014221 671649 dockerimagelookup.go:237] container image registry lookup failed: docker.io/username/image:latest: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit If so, you do not need [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/02/18/how-to-work-around-dockers-new-download-rate-limit-on-red-hat-openshift/"&gt;How to work around Docker&amp;#8217;s new download rate limit on Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/02/18/how-to-work-around-dockers-new-download-rate-limit-on-red-hat-openshift/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">870297</post-id><dc:creator>Joel Lord</dc:creator><dc:date>2021-02-18T08:00:17Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/02/18/how-to-work-around-dockers-new-download-rate-limit-on-red-hat-openshift/</feedburner:origLink></entry><entry><title type="html">VU Alumni Spotlight - Open Key to Every Career</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/xg_5DGILIUE/vu-alumni-spotlight-open-key-to-every-career.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/Tk5_1C1ZfjU/vu-alumni-spotlight-open-key-to-every-career.html</id><updated>2021-02-18T06:00:00Z</updated><content type="html">I've been invited to come back and give a talk at my university as part of their alumni spotlight series.  The funny thing is, this event has been in the planning since pre-pandemic days last year and was initially to be onsite and in person for everyone.  A year later and it's a different world where we still need to do these things virtually and online.  Next Thursday, 25 Feb 2021 from 16:00 - 17:00 I'll be sharing some motivational insights from my personal career journey since leaving the Vrije Universiteit in Amsterdam.  It's a talk that centers how being open can mean everything to your career. If you are interested, reach out and I'll share the online connection information for this session and we'll be talking about more of the following. The following will be shared online in a virtual event with anyone wanting to attend. OPEN IS KEY TO EVERY CAREER  It's not a coincidence. It's not just luck. It's not going to happen by itself, so what's the secret sauce for accelerating your career path? Understanding what makes a career grow, what choices are crucial, and what actions accelerate or damage your future are sometimes hard to grasp. Learning to position, expand and grow your personal brand in an open source world is what this session provides. Be ready for your next career step using open source principles. Join me for a story sharing a clear and easy to use plan for jump starting your career immediately. Again, reach out if you want to join, I'll share connection information closer to the event. Slides will be posted online on the day of the event.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/xg_5DGILIUE" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/Tk5_1C1ZfjU/vu-alumni-spotlight-open-key-to-every-career.html</feedburner:origLink></entry><entry><title type="html">Event-driven decisions with Kogito</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/3VoUhsFI34M/event-driven-decisions-with-kogito.html" /><author><name>Alessandro Costa</name></author><id>https://blog.kie.org/2021/02/event-driven-decisions-with-kogito.html</id><updated>2021-02-17T10:56:18Z</updated><content type="html">In 2021 it’s almost undeniable that modern application development needs to target the cloud, given the requirements of flexibility, scalability and availability imposed by today’s world. Event-driven architectures have proven to be well suited models for this purpose. As a result, we’re adopting these principles in several components of Kogito, which aims to be the next generation cloud-native business automation solution. This blogpost presents a new component that aligns with this view: the event-driven decisions addon. It is available since Kogito v1.2. KEY CONCEPTS This addon enables the evaluation of decision models in an event-driven fashion, so that it can be used as part of an event processing pipeline. It comes in two flavours: Quarkus and Spring Boot. The developer only needs to include the correct version as dependency of his Kogito app and configure the input/output topics. The wiring is done by the Kogito code-generation and framework specific dependency injection. The execution is triggered upon receiving an event containing the initial context from a specific Kafka input topic. The result is then sent to a Kafka output topic (which may be the same). Both input and output events are formatted as . Its capabilities are implemented to be similar to the ones available via REST endpoints: * trigger evaluation of the whole model or of a specific decision service * receive only the context or the full DMN result in the output event * filter out the inputs from the output context, no matter if returned alone or inside the DMN result EVENT STRUCTURE INPUT EVENT A model evaluation is triggered by a specific event called DecisionRequest. Here is the list of the supported fields, including the optional ones: FieldPurposeMandatoryDefaultdataInput contextyes–idCloudEvent IDyes–kogitodmnevaldecisionName of decision service to evaluate. If specified the engine triggers the evaluation of this service only.nonullkogitodmnfilteredctxBoolean flag to enable/disable filtering out inputs from the contextnofalsekogitodmnfullresultBoolean flag to enable/disable receiving full DMN result as outputnofalsekogitodmnmodelnameName of DMN model to evaluateyes–kogitodmnmodelnamespaceNamespace of DMN model to evaluateyes–sourceCloudEvent sourceyes–specversionMust be equal to 1.0 as mandated by CloudEvent specificationyes–subjectIf specified, the engine will put the same value as subject of the output event. Its usage is up to the caller (e.g. as correlation ID).nonulltypeMust be equal to DecisionRequestyes– EXAMPLE OF DECISIONREQUEST EVENT { "specversion": "1.0", "id": "a89b61a2-5644-487a-8a86-144855c5dce8", "source": "SomeEventSource", "type": "DecisionRequest", "kogitodmnmodelname": "Traffic Violation", "Kogitodmnmodelnamespace": "https://github.com/kiegroup/drools/kie-dmn/_A4BCA8B8-CF08-433F-93B2-A2598F19ECFF", "data": { "Driver": { "Age": 25, "Points": 13 }, "Violation": { "Type": "speed", "Actual Speed": 115, "Speed Limit": 100 } } } OUTPUT EVENTS If the request is evaluated successfully, the system returns two different types of output events depending on the value of the kogitodmnfullresult flag: * DecisionResponse if only the output context is returned * DecisionResponseFull if the full DMN result is returned The results are always in the data field. EXAMPLE OF DECISIONRESPONSE EVENT { "specversion": "1.0", "id": "d54ace84-6788-46b6-a359-b308f8b21778", "source": "Traffic+Violation", "type": "DecisionResponse", "kogitodmnmodelnamespace": "https://github.com/kiegroup/drools/kie-dmn/_A4BCA8B8-CF08-433F-93B2-A2598F19ECFF", "kogitodmnmodelname": "Traffic Violation", "data": { "Violation": { "Type": "speed", "Speed Limit": 100, "Actual Speed": 115 }, "calculateTotalPoints": "function calculateTotalPoints( driver, fine )", "Driver": { "Points": 13, "Age": 25 }, "Fine": { "Points": 3, "Amount": 500 }, "Should the driver be suspended?": "No" } } EXAMPLE OF DECISIONRESPONSEFULL EVENT { "specversion": "1.0", "id": "a18c409d-ab1f-4d6b-abc7-97327df8585f", "source": "Traffic+Violation", "type": "DecisionResponseFull", "kogitodmnmodelnamespace": "https://github.com/kiegroup/drools/kie-dmn/_A4BCA8B8-CF08-433F-93B2-A2598F19ECFF", "kogitodmnmodelname": "Traffic Violation", "data": { "namespace": "https://github.com/kiegroup/drools/kie-dmn/_A4BCA8B8-CF08-433F-93B2-A2598F19ECFF", "modelName": "Traffic Violation", "dmnContext": { "Violation": { "Type": "speed", "Speed Limit": 100, "Actual Speed": 115 }, "calculateTotalPoints": "function calculateTotalPoints( driver, fine )", "Driver": { "Points": 13, "Age": 25 }, "Fine": { "Points": 3, "Amount": 500 }, "Should the driver be suspended?": "No" }, "messages": [], "decisionResults": [ { "decisionId": "_4055D956-1C47-479C-B3F4-BAEB61F1C929", "decisionName": "Fine", "result": { "Points": 3, "Amount": 500 }, "messages": [], "evaluationStatus": "SUCCEEDED" }, { "decisionId": "_8A408366-D8E9-4626-ABF3-5F69AA01F880", "decisionName": "Should the driver be suspended?", "result": "No", "messages": [], "evaluationStatus": "SUCCEEDED" } ] } } ERROR EVENTS If, for some reason, the request event is malformed or contains wrong information so that the evaluation can’t be triggered, a DecisionResponseError is sent as output. In this case the data field contains a string that specifies the error type: Error TypeMeaningBAD_REQUESTMalformed input event (e.g. when some mandatory fields are missing)MODEL_NOT_FOUNDThe specified model can’t be found in the current service EXAMPLES The Kogito Examples repository contains two examples, and , that you can use as a starting point to practice with this addon. They also contain tests for every possible variation in the structure of the input/output events supported by the addon. CONCLUSION If you liked this article and are interested in the evolution of Kogito, stay tuned for more news! Thanks for reading. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/3VoUhsFI34M" height="1" width="1" alt=""/&gt;</content><dc:creator>Alessandro Costa</dc:creator><feedburner:origLink>https://blog.kie.org/2021/02/event-driven-decisions-with-kogito.html</feedburner:origLink></entry><entry><title type="html">RefCard - Getting started with OpenShift published</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/isucJI91iQw/refcard-getting-started-with-openshift.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/PJSsJ9oNvsU/refcard-getting-started-with-openshift.html</id><updated>2021-02-16T06:00:00Z</updated><content type="html">Earlier this month a writing project I was working on, a getting started with OpenShift reference card, went live .  The project was to put together a getting started guide that walks a developer through getting OpenShift, installing it on a local machine, and a quick start to using one of the provided operators.  Basically, providing the first steps any developer would need to get started experiencing cloud-native application development. Even better, it's using CodeReady Containers to allow any developer to follow along with this refcard and experience OpenShift on their own local developer machine. Everything shown in this refcard is freely available for download and the process followed has been put into a project that anyone can use. The entire document is eight pages and includes code examples to help you on your journey to exploring and using cloud-native development on a Kubernetes based platform. The best way to get started is to share a bit of the introduction and then you can head over to .  ABSTRACT Red Hat OpenShift is an enterprise open source container orchestration platform. It’s a software product that includes components of the Kubernetes container management project, but adds productivity and security features that are important to large-scale companies. In this Refcard, learn how to get started with the developer usage of Red Hat OpenShift to help you go hands-on with installing a local development cluster on your own machine. TABLE OF CONTENTS * Introduction * What Is OpenShift? * Installing OpenShift * Exploring the OpenShift Platform * Installing With Operators * Conclusion INTRODUCTION What is Red Hat’s OpenShift Container Platform, and what does it offer in the way of easier-to-use Kubernetes? It’s an enterprise-ready Kubernetes container platform with full-stack automated operations to manage hybrid cloud, multi-cloud, and edge deployments. No matter the cloud provider, with Red Hat OpenShift, you can give your developers an open-standards-based container platform on Kubernetes that not only runs your application workloads but provides cloud portability for those workloads.  This Refcard gets you started on developer usage of Red Hat OpenShift and helps you go hands-on with installing a local development cluster using CodeReady Containers on your own machine. After installation, you’ll see an example of deploying the available data protection and management solution using the operator available on the Operator Hub.  That's the preview so far, hope you like it enough to want to read the entire Refcard, if so .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/isucJI91iQw" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/PJSsJ9oNvsU/refcard-getting-started-with-openshift.html</feedburner:origLink></entry><entry><title type="html">Keycloak 12.0.3 released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/uAH8QdcS5Wk/keycloak-1203-released.html" /><author><name /></author><id>https://www.keycloak.org//2021/02/keycloak-1203-released.html</id><updated>2021-02-16T00:00:00Z</updated><content type="html">To download the release go to . ALL RESOLVED ISSUES The full list of resolved issues are available in UPGRADING Before you upgrade remember to backup your database and check the for anything that may have changed.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/uAH8QdcS5Wk" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://www.keycloak.org//2021/02/keycloak-1203-released.html</feedburner:origLink></entry><entry><title>Integrating Spring Boot with Red Hat Integration Service Registry</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/n4TfcdA_VnQ/" /><category term="Event-Driven" /><category term="Java" /><category term="Kubernetes" /><category term="Microservices" /><category term="Spring Boot" /><category term="amq streams" /><category term="Apicurio" /><category term="Apicurio Registry" /><category term="Red Hat Integration" /><category term="service registry" /><author><name>Roman Martin Gil</name></author><id>https://developers.redhat.com/blog/?p=779347</id><updated>2021-02-15T08:00:01Z</updated><published>2021-02-15T08:00:01Z</published><content type="html">&lt;p&gt;Most of the new cloud-native applications and &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; designs are based on &lt;a href="https://developers.redhat.com/topics/event-driven/"&gt;event-driven architecture&lt;/a&gt; (EDA), responding to real-time information by sending and receiving information about individual events. This kind of architecture relies on asynchronous, non-blocking communication between event producers and consumers through an event streaming backbone such as &lt;a href="https://developers.redhat.com/blog/2019/12/04/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-1/"&gt;Red Hat AMQ Streams&lt;/a&gt; running on top of &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. In scenarios where many different events are being managed, defining a governance model where each event is defined as an API is critical. That way, producers and consumers can produce and consume checked and validated events. We can use a service registry as a datastore for events defined as APIs.&lt;/p&gt; &lt;p&gt;From my field experience working with many clients, I&amp;#8217;ve found the most typical architecture consists of the following components:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMQ Streams to deploy &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt; clusters as the streaming backbone.&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://www.apicur.io/registry/"&gt;Apicurio Registry&lt;/a&gt; for storing events as APIs.&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/courses/openshift/getting-started"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; to deploy and run the different components.&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/topics/spring-boot"&gt;Spring Boot&lt;/a&gt; as the framework for developing &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; client applications.&lt;/li&gt; &lt;li&gt;&lt;a target="_blank" rel="nofollow" href="https://avro.apache.org/"&gt;Avro&lt;/a&gt; as the data serialization system to declare schemas as event APIs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In this article, you will learn how to easily integrate your Spring Boot applications with &lt;a href="https://developers.redhat.com/blog/2019/12/16/getting-started-with-red-hat-integration-service-registry/"&gt;Red Hat Integration Service Registry&lt;/a&gt;, which is based on the open source &lt;a target="_blank" rel="nofollow" href="https://github.com/apicurio/apicurio-registry"&gt;Apicurio Registry&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;span id="more-779347"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Red Hat Integration Service Registry&lt;/h2&gt; &lt;p&gt;Service Registry is a datastore for sharing standard event schemas and API designs across APIs and event-driven architectures. Service Registry decouples the structure of your data from your client applications so that you can share and manage your data types and API descriptions at runtime. It also reduces costs by decreasing the overall message size, and it creates efficiencies by increasing the consistent reuse of schemas and API designs across your organization.&lt;/p&gt; &lt;p&gt;Some of the most common use cases for Service Registry are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Client applications that dynamically push or pull the latest schema updates to or from the service registry at runtime without redeploying.&lt;/li&gt; &lt;li&gt;Developer teams that query the registry for existing schemas required for services already deployed in production.&lt;/li&gt; &lt;li&gt;Developer teams that register new schemas required for new services in development or rolling to production.&lt;/li&gt; &lt;li&gt;Stored schemas used to serialize and deserialize messages. Client applications can reference the stored schemas to ensure the messages they send and receive are compatible with the schemas.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Service Registry provides the following main features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Support for multiple payload formats for standard event schemas and API specifications.&lt;/li&gt; &lt;li&gt;Pluggable storage options, including AMQ Streams, an embedded Infinispan in-memory data grid, and PostgreSQL database.&lt;/li&gt; &lt;li&gt;Registry content management using a web console, REST API commands, Maven plug-ins, or a Java client.&lt;/li&gt; &lt;li&gt;Rules for content validation and version compatibility to govern how registry content evolves.&lt;/li&gt; &lt;li&gt;Support for the Apache Kafka schema registry, including &lt;a href="https://developers.redhat.com/blog/2020/02/14/using-secrets-in-apache-kafka-connect-configuration/"&gt;Kafka Connect&lt;/a&gt; integration for external systems.&lt;/li&gt; &lt;li&gt;A client serializer/deserializer (&lt;a target="_blank" rel="nofollow" href="https://en.wikipedia.org/wiki/SerDes"&gt;SerDes&lt;/a&gt;) to validate Kafka and other message types at runtime.&lt;/li&gt; &lt;li&gt;A cloud-native &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus Java runtime&lt;/a&gt; for low memory footprint and fast deployment times.&lt;/li&gt; &lt;li&gt;Compatibility with existing Confluent schema registry client applications.&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/topics/kubernetes/operators"&gt;Operator&lt;/a&gt;-based installation on OpenShift.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Introducing Service Registry for client applications&lt;/h2&gt; &lt;p&gt;The typical workflow for introducing a new service registry to our architecture is to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Declare the event schema using common data formats like Apache Avro, a JSON schema, Google Protocol Buffers, OpenAPI, AsyncAPI, GraphQL, Kafka Connect schemas, WSDL, or XML schemas (XSD).&lt;/li&gt; &lt;li&gt;Register the schema as an artifact in the service registry using the &lt;a href="https://developers.redhat.com/blog/2020/06/11/first-look-at-the-new-apicurio-registry-ui-and-operator/"&gt;Service Registry UI&lt;/a&gt;, REST API, Maven plug-in, or a Java client. Client applications can then use the schema to validate that messages conform to the correct data structure at runtime.&lt;/li&gt; &lt;li&gt;Use Kafka producer applications and serializers to encode messages that conform to a specific event schema.&lt;/li&gt; &lt;li&gt;Use Kafka consumer applications and deserializers to validate that messages have been serialized using the correct schema based on a specific schema ID.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This workflow ensures consistent schema use and helps to prevent data errors at runtime.&lt;/p&gt; &lt;p&gt;The next sections discuss this workflow at a high level using a Spring Boot application. See the article&amp;#8217;s GitHub repository for the &lt;a target="_blank" rel="nofollow" href="https://github.com/rmarting/kafka-clients-sb-sample"&gt;complete sample application source code&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Register an Avro schema in Service Registry&lt;/h2&gt; &lt;p&gt;Avro provides a &lt;a target="_blank" rel="nofollow" href="https://avro.apache.org/docs/current/spec.html#schemas"&gt;JSON schema specification&lt;/a&gt; to declare a variety of data structures. This simple example defines a message event:&lt;/p&gt; &lt;pre&gt;{ "name": "Message", "namespace": "com.rmarting.kafka.schema.avro", "type": "record", "doc": "Schema for a Message.", "fields": [ { "name": "timestamp", "type": "long", "doc": "Message timestamp." }, { "name": "content", "type": "string", "doc": "Message content." } ] }&lt;/pre&gt; &lt;p&gt;Avro also provides a &lt;a target="_blank" rel="nofollow" href="https://avro.apache.org/docs/current/gettingstartedjava.html"&gt;Maven plug-in&lt;/a&gt; to autogenerate Java classes based on the provided schema definitions (.avsc files).&lt;/p&gt; &lt;p&gt;Once we have a schema, we can publish it in Service Registry. Publishing a schema in the registry makes it ready for client applications to use at runtime. The &lt;a target="_blank" rel="nofollow" href="https://www.apicur.io/registry/docs/apicurio-registry/1.3.3.Final/getting-started/assembly-managing-registry-artifacts-maven.html"&gt;Apicurio Registry Maven plug-in&lt;/a&gt; makes it easy to publish our schema to Service Registry. We add a simple definition in our &lt;code&gt;pom.xml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&amp;#60;plugin&amp;#62; &amp;#60;groupId&amp;#62;io.apicurio&amp;#60;/groupId&amp;#62; &amp;#60;artifactId&amp;#62;apicurio-registry-maven-plugin&amp;#60;/artifactId&amp;#62; &amp;#60;version&amp;#62;${apicurio.version}&amp;#60;/version&amp;#62; &amp;#60;executions&amp;#62; &amp;#60;execution&amp;#62; &amp;#60;phase&amp;#62;generate-sources&amp;#60;/phase&amp;#62; &amp;#60;goals&amp;#62; &amp;#60;goal&amp;#62;register&amp;#60;/goal&amp;#62; &amp;#60;/goals&amp;#62; &amp;#60;configuration&amp;#62; &amp;#60;registryUrl&amp;#62;${apicurio.registry.url}&amp;#60;/registryUrl&amp;#62; &amp;#60;artifactType&amp;#62;AVRO&amp;#60;/artifactType&amp;#62; &amp;#60;artifacts&amp;#62; &amp;#60;!-- Schema definition for TopicIdStrategy strategy --&amp;#62; &amp;#60;messages-value&amp;#62;${project.basedir}/src/main/resources/schemas/message.avsc&amp;#60;/messages-value&amp;#62; &amp;#60;/artifacts&amp;#62; &amp;#60;/configuration&amp;#62; &amp;#60;/execution&amp;#62; &amp;#60;/executions&amp;#62; &amp;#60;/plugin&amp;#62;&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: With the Apicurio Registry Maven plug-in, we could use the &lt;a target="_blank" rel="nofollow" href="https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html"&gt;Maven build lifecycle&lt;/a&gt; to define or extend our application lifecycle management and &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt; pipelines. For example, we could extend the lifecycle to publish or update schemas whenever a new version was released. Explaining how to do it is not the objective of this article, but it&amp;#8217;s something you could explore.&lt;/p&gt; &lt;p&gt;As soon as we publish our schema to Service Registry, we can manage it from the Service Registry UI shown in Figure 1.&lt;/p&gt; &lt;div id="attachment_779587" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2020/09/img_5f58c2828d45e.png"&gt;&lt;img aria-describedby="caption-attachment-779587" class="wp-image-779587 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2020/09/img_5f58c2828d45e-1024x681.png" alt="The Service Registry user interface." width="640" height="426" srcset="https://developers.redhat.com/blog/wp-content/uploads/2020/09/img_5f58c2828d45e-1024x681.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/img_5f58c2828d45e-300x200.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/img_5f58c2828d45e-768x511.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2020/09/img_5f58c2828d45e.png 1102w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-779587" class="wp-caption-text"&gt;Figure 1: Manage your new schema in the Service Registry UI.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Integrate Spring Boot, Apache Kafka, and AMQ Streams&lt;/h2&gt; &lt;p&gt;Spring Boot provides the &lt;a target="_blank" rel="nofollow" href="https://spring.io/projects/spring-kafka"&gt;Spring Kafka&lt;/a&gt; project to produce and consume messages to and from Apache Kafka. Using it is straightforward once we add the following dependency in our &lt;code&gt;pom.xml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&amp;#60;dependency&amp;#62; &amp;#60;groupId&amp;#62;org.springframework.kafka&amp;#60;/groupId&amp;#62; &amp;#60;artifactId&amp;#62;spring-kafka&amp;#60;/artifactId&amp;#62; &amp;#60;/dependency&amp;#62;&lt;/pre&gt; &lt;p&gt;Adding the following property to your &lt;code&gt;application.properties&lt;/code&gt; file connects your application with the AMQ Streams cluster:&lt;/p&gt; &lt;pre&gt;spring.kafka.bootstrap-servers = ${kafka.bootstrap-servers}&lt;/pre&gt; &lt;p&gt;Service Registry provides &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_integration/2020-q2/html-single/getting_started_with_service_registry/index#using-kafka-client-serdes"&gt;Kafka client serializers/deserializers&lt;/a&gt; for Kafka producer and consumer applications. Include the following dependency to add them to your application:&lt;/p&gt; &lt;pre&gt;&amp;#60;dependency&amp;#62; &amp;#60;groupId&amp;#62;io.apicurio&amp;#60;/groupId&amp;#62; &amp;#60;artifactId&amp;#62;apicurio-registry-utils-serde&amp;#60;/artifactId&amp;#62; &amp;#60;version&amp;#62;${apicurio.version}&amp;#60;/version&amp;#62; &amp;#60;/dependency&amp;#62;&lt;/pre&gt; &lt;h2&gt;Produce messages from Spring Boot&lt;/h2&gt; &lt;p&gt;Spring Kafka provides a set of properties and beans to declare Kafka producers to send messages (Avro schema instances, in this case) to Apache Kafka. The two most important properties are &lt;code&gt;spring.kafka.producer.key-serializer&lt;/code&gt;, which identifies the serializer class to serialize the Kafka record&amp;#8217;s key, and &lt;code&gt;spring.kafka.producer.value-serializer&lt;/code&gt;, which identifies the serializer class to serialize the Kafka record&amp;#8217;s value.&lt;/p&gt; &lt;p&gt;We have to add specific values in these properties so that the serialization process using Avro schemas can be registered in Service Registry:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The serializer class to use Avro schemas provided by the Apicurio SerDe class: &lt;code&gt;io.apicurio.registry.utils.serde.AvroKafkaSerializer&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;The Apicurio service registry endpoint to validate schemas: &lt;code&gt;apicurio.registry.url&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;The &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_integration/2020-q2/html-single/getting_started_with_service_registry/index#service-registry-concepts-strategy-service-registry"&gt;Apicurio service strategy&lt;/a&gt; to look up the schema definition: &lt;code&gt;apicurio.registry.artifact-id&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here&amp;#8217;s a sample configuration for a producer template:&lt;/p&gt; &lt;pre&gt;# Spring Kafka Producer spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer spring.kafka.producer.value-serializer=io.apicurio.registry.utils.serde.AvroKafkaSerializer spring.kafka.producer.properties.apicurio.registry.url = ${apicurio.registry.url} spring.kafka.producer.properties.apicurio.registry.artifact-id = io.apicurio.registry.utils.serde.strategy.TopicIdStrategy&lt;/pre&gt; &lt;p&gt;We can use the following properties to declare a &lt;code&gt;KafkaTemplate&lt;/code&gt; to send messages (based on our &lt;code&gt;Message&lt;/code&gt; schema):&lt;/p&gt; &lt;pre&gt;@Bean public ProducerFactory&amp;#60;String, Message&amp;#62; producerFactory(KafkaProperties kafkaProperties) { Map&amp;#60;String, Object&amp;#62; configProps = kafkaProperties.buildProducerProperties(); return new DefaultKafkaProducerFactory&amp;#60;&amp;#62;(configProps); } @Bean public KafkaTemplate&amp;#60;String, Message&amp;#62; kafkaTemplate(KafkaProperties kafkaProperties) { return new KafkaTemplate&amp;#60;&amp;#62;(producerFactory(kafkaProperties)); }&lt;/pre&gt; &lt;p&gt;Finally, we can send messages (storing the artifact ID from Service Registry) to Apache Kafka:&lt;/p&gt; &lt;pre&gt;@Autowired private KafkaTemplate&amp;#60;String, Message&amp;#62; kafkaTemplate; SendResult&amp;#60;String, Message&amp;#62; record = kafkaTemplate.send(topicName, message).get();&lt;/pre&gt; &lt;p&gt;The message will be serialized, adding the global ID associated with the schema used for this record. It will be important for the Kafka consumer applications to consume the global ID later.&lt;/p&gt; &lt;h2&gt;Consume messages from Spring Boot&lt;/h2&gt; &lt;p&gt;Spring Kafka also provides properties and beans to declare Kafka consumers to consume messages (Avro schema instances) from the Apache Kafka cluster. The most important properties are &lt;code&gt;spring.kafka.consumer.key-deserializer&lt;/code&gt;, which identifies the deserializer class to deserialize the Kafka record&amp;#8217;s key, and &lt;code&gt;spring.kafka.consumer.value-deserializer&lt;/code&gt;, which identifies the deserializer class to deserialize the Kafka record&amp;#8217;s value.&lt;/p&gt; &lt;p&gt;Once again, we have to add specific values—in this case, to allow the deserialization process using Avro schemas registered in Service Registry:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The deserializer class to use Avro schemas, which is provided by the Apicurio SerDe class: &lt;code&gt;io.apicurio.registry.utils.serde.AvroKafkaDeserializer&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Th Apicurio service registry endpoint to get valid schemas: &lt;code&gt;apicurio.registry.url&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here&amp;#8217;s a sample configuration for a consumer template:&lt;/p&gt; &lt;pre&gt;# Spring Kafka Consumer spring.kafka.listener.ack-mode = manual spring.kafka.consumer.group-id = spring-kafka-clients-sb-sample-group spring.kafka.consumer.auto-offset-reset = earliest spring.kafka.consumer.enable-auto-commit=false spring.kafka.consumer.key-deserializer = org.apache.kafka.common.serialization.StringDeserializer spring.kafka.consumer.value-deserializer = io.apicurio.registry.utils.serde.AvroKafkaDeserializer spring.kafka.consumer.properties.apicurio.registry.url = ${apicurio.registry.url} # Use Specific Avro classes instead of the GenericRecord class definition spring.kafka.consumer.properties.apicurio.registry.use-specific-avro-reader = true&lt;/pre&gt; &lt;p&gt;We can declare a &lt;code&gt;KafkaListener&lt;/code&gt; to consume messages (based on our &lt;code&gt;Message&lt;/code&gt; schema) as:&lt;/p&gt; &lt;pre&gt;@KafkaListener(topics = {"messages"}) public void handleMessages(@Payload Message message, @Headers Map&amp;#60;String, Object&amp;#62; headers, Acknowledgment acknowledgment) { LOGGER.info("Received record from Topic-Partition '{}-{}' with Offset '{}' -&amp;#62; Key: '{}' - Value '{}'", headers.get(KafkaHeaders.RECEIVED_TOPIC), headers.get(KafkaHeaders.RECEIVED_PARTITION_ID), headers.get(KafkaHeaders.OFFSET), headers.get(KafkaHeaders.MESSAGE_KEY), message.get("content")); // Commit message acknowledgment.acknowledge(); }&lt;/pre&gt; &lt;p&gt;The schema is retrieved by the deserializer, which uses the global ID written into the message being consumed. With that, we&amp;#8217;re done!&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;In this article, you&amp;#8217;ve seen how to integrate Spring Boot applications with Red Hat Integration Service Registry and AMQ Streams to build your event-driven architecture. Using these components together gives you the following benefits:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Consistent schema use between client applications.&lt;/li&gt; &lt;li&gt;Help with preventing data errors at runtime.&lt;/li&gt; &lt;li&gt;A defined governance model in your data schemas (such as for versions, rules, and validations).&lt;/li&gt; &lt;li&gt;Easy integration with Java client applications and components.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;See the &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_integration/2020-q2/"&gt;Red Hat Integration&lt;/a&gt; homepage and &lt;a target="_blank" rel="nofollow" href="https://access.redhat.com/documentation/en-us/red_hat_integration/2020-q2/html/getting_started_with_service_registry/index"&gt;Service Registry documentation&lt;/a&gt; for more about these components. Also, see &lt;a href="https://developers.redhat.com/blog/2020/06/11/first-look-at-the-new-apicurio-registry-ui-and-operator/"&gt;&lt;i&gt;First look at the new Apicurio Registry UI and Operator&lt;/i&gt;&lt;/a&gt; for more about using Service Registry in any &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; or OpenShift cluster.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F15%2Fintegrating-spring-boot-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Integrating%20Spring%20Boot%20with%20Red%20Hat%20Integration%20Service%20Registry" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F15%2Fintegrating-spring-boot-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Integrating%20Spring%20Boot%20with%20Red%20Hat%20Integration%20Service%20Registry" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F15%2Fintegrating-spring-boot-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Integrating%20Spring%20Boot%20with%20Red%20Hat%20Integration%20Service%20Registry" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F15%2Fintegrating-spring-boot-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Integrating%20Spring%20Boot%20with%20Red%20Hat%20Integration%20Service%20Registry" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F15%2Fintegrating-spring-boot-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Integrating%20Spring%20Boot%20with%20Red%20Hat%20Integration%20Service%20Registry" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F15%2Fintegrating-spring-boot-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Integrating%20Spring%20Boot%20with%20Red%20Hat%20Integration%20Service%20Registry" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F15%2Fintegrating-spring-boot-with-red-hat-integration-service-registry%2F&amp;#38;linkname=Integrating%20Spring%20Boot%20with%20Red%20Hat%20Integration%20Service%20Registry" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F15%2Fintegrating-spring-boot-with-red-hat-integration-service-registry%2F&amp;#038;title=Integrating%20Spring%20Boot%20with%20Red%20Hat%20Integration%20Service%20Registry" data-a2a-url="https://developers.redhat.com/blog/2021/02/15/integrating-spring-boot-with-red-hat-integration-service-registry/" data-a2a-title="Integrating Spring Boot with Red Hat Integration Service Registry"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/02/15/integrating-spring-boot-with-red-hat-integration-service-registry/"&gt;Integrating Spring Boot with Red Hat Integration Service Registry&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/n4TfcdA_VnQ" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Most of the new cloud-native applications and microservices designs are based on event-driven architecture (EDA), responding to real-time information by sending and receiving information about individual events. This kind of architecture relies on asynchronous, non-blocking communication between event producers and consumers through an event streaming backbone such as Red Hat AMQ Streams running on top [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/02/15/integrating-spring-boot-with-red-hat-integration-service-registry/"&gt;Integrating Spring Boot with Red Hat Integration Service Registry&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/02/15/integrating-spring-boot-with-red-hat-integration-service-registry/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">779347</post-id><dc:creator>Roman Martin Gil</dc:creator><dc:date>2021-02-15T08:00:01Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/02/15/integrating-spring-boot-with-red-hat-integration-service-registry/</feedburner:origLink></entry><entry><title type="html">Infinispan 12.0.1.Final “Lockdown”</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/__7eKygf9Nc/infinispan-12-0-1" /><author><name>Ryan Emerson</name></author><id>/blog/2021/02/12/infinispan-12-0-1</id><updated>2021-02-12T12:00:00Z</updated><content type="html">Dear Infinispan community, we hope you’ve been enjoying all the new goodies included in our latest major release, Infinispan 12. We now have a brand new micro release for you which addresses a number of issues. The following list shows what we have fixed: COMPONENT UPGRADE * - Upgrade console to 0.13.1.Final ENHANCEMENT * - Implement proper form login and digest login * - Implicit connectors for the single-port endpoint * - Add addtional DEBUG logs to Backup/Restore operations FEATURE REQUEST * - Add property to disable test execution BUG * - Replicated cache get ignores value in zero-capacity nodes * - CacheEntryCloudEventsTest test failures * - Get entry broken in protobuf caches * - Fix IracWriteSkewTest test * - StatsTest random failures * - Licenses are not properly generated in console * - Functional commands break in replicated caches on zero-capacity nodes * - LocalIndexSyncStateTransferTest random failures * - ReplicationIndexTest random failures * - GlobalState incompatibility between 11.x and 12.x * - Tests failing due to relying on published images * - CLI Batch files returns exit code 0 on error * - Cannot retrieve the keyset of a cache (ISPN000287) * - IllegalArgumentException in VoidResponseCollector: Self-suppression not permitted * - Hot Rod iteration shouldn’t require ADMIN permission * - When a value is json but not protobuf the console fails * - Hot Rod client has too many Elytron dependencies TASK * - Upgrade logo in Operator Hub&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/__7eKygf9Nc" height="1" width="1" alt=""/&gt;</content><dc:creator>Ryan Emerson</dc:creator><feedburner:origLink>http://tools.jboss.org/blog/2021/02/12/infinispan-12-0-1</feedburner:origLink></entry><entry><title>Developing your own custom devfiles for odo 2.0</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/p2RepYy6GgU/" /><category term="Containers" /><category term="Developer Tools" /><category term="Event-Driven" /><category term="Java" /><category term="Kubernetes" /><category term="custom devfile" /><category term="devfiles" /><category term="odo" /><category term="openshift" /><category term="poststart" /><author><name>Gorkem Ercan</name></author><id>https://developers.redhat.com/blog/?p=836997</id><updated>2021-02-12T08:00:17Z</updated><published>2021-02-12T08:00:17Z</published><content type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/odo/overview"&gt;Odo 2.0&lt;/a&gt; introduces a configuration file named &lt;code&gt;devfile.yaml&lt;/code&gt;. Odo uses this configuration file to set up cloud-native projects and determine the actions required for events such as building, running, and debugging a project. If you are an &lt;a target="_blank" rel="nofollow" href="https://www.eclipse.org/che/"&gt;Eclipse Che&lt;/a&gt; user, &lt;code&gt;devfile.yaml&lt;/code&gt; should sound familiar: Eclipse Che uses devfiles to express developer workspaces, and they have proven to be flexible to accommodate a variety of needs.&lt;/p&gt; &lt;p&gt;Odo 2.0 comes with a built-in catalog of devfiles for various project types, so you do not necessarily need to write or modify a devfile to start a new project. You can also create custom devfiles and contribute them to odo&amp;#8217;s devfile catalog. This article explores how to create a devfile to adopt an existing development flow to run on a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; cluster. Our example project is based on &lt;a target="_blank" rel="nofollow" href="https://www.gatsbyjs.com/"&gt;Gatsby&lt;/a&gt;, a framework for generating websites. Gatsby comes with its own developer tools and recommended development flow, so it presents a good example for adopting existing flows for Kubernetes.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;&lt;strong&gt;Note&lt;/strong&gt;: See &lt;em&gt;&lt;a href="https://developers.redhat.com/blog/2020/10/06/kubernetes-integration-and-more-in-odo-2-0/"&gt;Kubernetes integration and more in odo 2.0&lt;/a&gt;&lt;/em&gt; for more about devfiles and other new features in this latest release.&lt;/p&gt; &lt;h2&gt;Anatomy of a devfile&lt;/h2&gt; &lt;p&gt;Before we begin working with the example, let’s take a quick look at the anatomy of a devfile.&lt;/p&gt; &lt;p&gt;Other than the &lt;code&gt;schemaVersion&lt;/code&gt;, no other properties are mandatory on a devfile. This flexibility lets developers use devfiles for multiple purposes. A devfile can be generic for a technology base or specific to a project, and projects can inherit and override parts of other devfiles.&lt;/p&gt; &lt;p&gt;Components, commands, and events are the most commonly used devfile properties:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;b&gt;Components&lt;/b&gt; describe the parts of the development environment that need to be created. Examples include runtime containers and Kubernetes resources.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Commands&lt;/b&gt; describe the predefined commands to be used to achieve specific development goals with the provided tools.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Events&lt;/b&gt; bind commands to the lifecycle of the developer environment. Currently, there are four events: &lt;code&gt;postStart&lt;/code&gt;, &lt;code&gt;postStop&lt;/code&gt;, &lt;code&gt;preStart&lt;/code&gt;, and &lt;code&gt;preStop&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Implement a devfile&lt;/h2&gt; &lt;p&gt;You&amp;#8217;ve had a quick introduction to devfiles and their three most commonly used properties. Now, let&amp;#8217;s apply what you&amp;#8217;ve learned. For this example, we will project instructions from the &lt;a target="_blank" rel="nofollow" href="https://www.gatsbyjs.com/docs/"&gt;Gatsby documentation&lt;/a&gt; into a devfile, which we&amp;#8217;ll use to develop a website on Kubernetes.&lt;/p&gt; &lt;h3&gt;Select a base image&lt;/h3&gt; &lt;p&gt;Because the application runs as a container, we&amp;#8217;ll start by selecting a base image and defining it as a component:&lt;/p&gt; &lt;pre&gt;schemaVersion: 2.0.0 components:   - name: gatsby container:    image: quay.io/eclipse/che-nodejs10-ubi:nightly    mountSources: true    memoryLimit: 700Mi    endpoints:      - name: web        targetPort: 8000 &lt;/pre&gt; &lt;p&gt;It’s worth mentioning the container&amp;#8217;s &lt;code&gt;mountSources&lt;/code&gt; property. Odo uses this value as a hint for synchronizing your local files to the container running on your Kubernetes cluster.&lt;/p&gt; &lt;h3&gt;Define the commands&lt;/h3&gt; &lt;p&gt;Next, let’s define the commands that we&amp;#8217;ll use to build and run the application. The two commands that we need to define will run on the application&amp;#8217;s &lt;code&gt;gatsby&lt;/code&gt; component. The &lt;code&gt;gatsby-develop&lt;/code&gt; command starts the application in development mode. The &lt;code&gt;setup-gatsby-cli&lt;/code&gt; command sets up Gatsby&amp;#8217;s development tools on the &lt;code&gt;gatsby&lt;/code&gt; component:&lt;/p&gt; &lt;pre&gt;commands:   - id: gatsby-develop exec:    commandLine: "gatsby develop -H 0.0.0.0"    component: gatsby    group:      kind: run    attributes:      restart: "false"   - id: setup-gatsby-cli exec:    commandLine: "npm install -g gatsby-cli &amp;#38;&amp;#38; npm install"    component: gatsby &lt;/pre&gt; &lt;h3&gt;Define the events&lt;/h3&gt; &lt;p&gt;Finally, we define a &lt;code&gt;postStart&lt;/code&gt; event to optimize the setup once a component has started:&lt;/p&gt; &lt;pre&gt;events:   postStart: - setup-gatsby-cli &lt;/pre&gt; &lt;h2&gt;Create and push the example application&lt;/h2&gt; &lt;p&gt;Assuming you have odo and the Gatsby CLI installed locally, you can put your newly acquired devfile to work. Here are the commands to create and push a simple Gatsby site using odo:&lt;/p&gt; &lt;pre&gt;gatsby new hello-world https://github.com/gatsbyjs/gatsby-starter-hello-world cd hello-world ## create or copy devfile.yaml ## from https://gist.github.com/gorkem/78fd17864218a125b2bd9146728a1af8 odo push &lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;While odo comes with a built-in catalog of devfiles, you can also develop your own. Creating custom devfiles lets you integrate the technologies that you work with into the Kubernetes environment. Once you&amp;#8217;ve created a devfile, you can contribute it to the devfile catalog for wider community reach.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F12%2Fdeveloping-your-own-custom-devfiles-for-odo-2-0%2F&amp;#38;linkname=Developing%20your%20own%20custom%20devfiles%20for%20odo%202.0" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F12%2Fdeveloping-your-own-custom-devfiles-for-odo-2-0%2F&amp;#38;linkname=Developing%20your%20own%20custom%20devfiles%20for%20odo%202.0" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F12%2Fdeveloping-your-own-custom-devfiles-for-odo-2-0%2F&amp;#38;linkname=Developing%20your%20own%20custom%20devfiles%20for%20odo%202.0" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F12%2Fdeveloping-your-own-custom-devfiles-for-odo-2-0%2F&amp;#38;linkname=Developing%20your%20own%20custom%20devfiles%20for%20odo%202.0" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F12%2Fdeveloping-your-own-custom-devfiles-for-odo-2-0%2F&amp;#38;linkname=Developing%20your%20own%20custom%20devfiles%20for%20odo%202.0" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F12%2Fdeveloping-your-own-custom-devfiles-for-odo-2-0%2F&amp;#38;linkname=Developing%20your%20own%20custom%20devfiles%20for%20odo%202.0" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F12%2Fdeveloping-your-own-custom-devfiles-for-odo-2-0%2F&amp;#38;linkname=Developing%20your%20own%20custom%20devfiles%20for%20odo%202.0" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2021%2F02%2F12%2Fdeveloping-your-own-custom-devfiles-for-odo-2-0%2F&amp;#038;title=Developing%20your%20own%20custom%20devfiles%20for%20odo%202.0" data-a2a-url="https://developers.redhat.com/blog/2021/02/12/developing-your-own-custom-devfiles-for-odo-2-0/" data-a2a-title="Developing your own custom devfiles for odo 2.0"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/02/12/developing-your-own-custom-devfiles-for-odo-2-0/"&gt;Developing your own custom devfiles for odo 2.0&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/p2RepYy6GgU" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Odo 2.0 introduces a configuration file named devfile.yaml. Odo uses this configuration file to set up cloud-native projects and determine the actions required for events such as building, running, and debugging a project. If you are an Eclipse Che user, devfile.yaml should sound familiar: Eclipse Che uses devfiles to express developer workspaces, and they have [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2021/02/12/developing-your-own-custom-devfiles-for-odo-2-0/"&gt;Developing your own custom devfiles for odo 2.0&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2021/02/12/developing-your-own-custom-devfiles-for-odo-2-0/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">836997</post-id><dc:creator>Gorkem Ercan</dc:creator><dc:date>2021-02-12T08:00:17Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/02/12/developing-your-own-custom-devfiles-for-odo-2-0/</feedburner:origLink></entry></feed>
